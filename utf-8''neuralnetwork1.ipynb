{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport time\nimport turicreate as tc\nfrom sklearn.model_selection import train_test_split\n\nimport sys\nsys.path.append(\"..\")\n\ncustomers = pd.read_csv('recommend_1.csv') \ntransactions = pd.read_csv('trx_data.csv')\ntransactions['products'] = transactions['products'].apply(lambda x: [int(i) for i in x.split('|')])",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pd.melt(transactions.set_index('customerId')['products'].apply(pd.Series).reset_index(), \n             id_vars=['customerId'],\n             value_name='products') \\\n    .dropna().drop(['variable'], axis=1) \\\n    .groupby(['customerId', 'products']) \\\n    .agg({'products': 'count'}) \\\n    .rename(columns={'products': 'purchase_count'}) \\\n    .reset_index() \\\n    .rename(columns={'products': 'productId'})\ndata['productId'] = data['productId'].astype(np.int64)\n\ndef create_data_dummy(data):\n    data_dummy = data.copy()\n    data_dummy['purchase_dummy'] = 1\n    return data_dummy\ndata_dummy = create_data_dummy(data)\n\ndf_matrix = pd.pivot_table(data, values='purchase_count', index='customerId', columns='productId')\ndf_matrix_norm = (df_matrix-df_matrix.min())/(df_matrix.max()-df_matrix.min())\n\n# create a table for input to the modeling  \nd = df_matrix_norm.reset_index() \nd.index.names = ['scaled_purchase_freq'] \ndata_norm = pd.melt(d, id_vars=['customerId'], value_name='scaled_purchase_freq').dropna()\n\n\ndef split_data(data):\n    '''\n    Splits dataset into training and test set.\n    \n    Args:\n        data (pandas.DataFrame)\n        \n    Returns\n        train_data (tc.SFrame)\n        test_data (tc.SFrame)\n    '''\n    train, test = train_test_split(data, test_size = .2)\n    train_data = tc.SFrame(train)\n    test_data = tc.SFrame(test)\n    return train_data, test_data\n\ntrain_data, test_data = split_data(data)\ntrain_data_dummy, test_data_dummy = split_data(data_dummy)\ntrain_data_norm, test_data_norm = split_data(data_norm)",
      "execution_count": 50,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "user_id = 'customerId'\nitem_id = 'productId'\nusers_to_recommend = list(customers[user_id])\nn_rec = 10 # number of items to recommend\nn_display = 30 # to display the first few rows in an output dataset\n\ndef model(train_data, name, user_id, item_id, target, users_to_recommend, n_rec, n_display):\n    if name == 'popularity':\n        model = tc.popularity_recommender.create(train_data, \n                                                    user_id=user_id, \n                                                    item_id=item_id, \n                                                    target=target)\n    elif name == 'cosine':\n        model = tc.item_similarity_recommender.create(train_data, \n                                                    user_id=user_id, \n                                                    item_id=item_id, \n                                                    target=target, \n                                                    similarity_type='cosine')\n    elif name == 'pearson':\n        model = tc.item_similarity_recommender.create(train_data, \n                                                    user_id=user_id, \n                                                    item_id=item_id, \n                                                    target=target, \n                                                    similarity_type='pearson')\n        \n        recom = model.recommend(users=users_to_recommend, k=n_rec)\n        recom.print_rows(n_display)\n        return model",
      "execution_count": 51,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "name = 'popularity1'\ntarget = 'purchase_count'\npopularity_model = model(train_data, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)\n\n\nname = 'popularity2'\ntarget = 'purchase_dummy'\npop_dummy = model(train_data_dummy, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)\n\n\nname = 'popularity3'\ntarget = 'scaled_purchase_freq'\npop_norm = model(train_data_norm, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)\n",
      "execution_count": 52,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "name = 'cosine1'\ntarget = 'purchase_count'\ncos = model(train_data, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)\n\nname = 'cosine2'\ntarget = 'purchase_dummy'\ncos_dummy = model(train_data_dummy, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)\n\nname = 'cosine3' \ntarget = 'scaled_purchase_freq' \ncos_norm = model(train_data_norm, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "name = 'pearson1'\ntarget = 'purchase_count'\npear = model(train_data, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)\n\nname = 'pearson2'\ntarget = 'purchase_dummy'\npear_dummy = model(train_data_dummy, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)\n\nname = 'pearson3'\ntarget = 'scaled_purchase_freq'\npear_norm = model(train_data_norm, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)\n\n",
      "execution_count": 54,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "models_w_counts = [popularity_model, cos, pear]\nmodels_w_dummy = [pop_dummy, cos_dummy, pear_dummy]\nmodels_w_norm = [pop_norm, cos_norm, pear_norm]\nnames_w_counts = ['Popularity Model on Purchase Counts', 'Cosine Similarity on Purchase Counts', 'Pearson Similarity on Purchase Counts']\nnames_w_dummy = ['Popularity Model on Purchase Dummy', 'Cosine Similarity on Purchase Dummy', 'Pearson Similarity on Purchase Dummy']\nnames_w_norm = ['Popularity Model on Scaled Purchase Counts', 'Cosine Similarity on Scaled Purchase Counts', 'Pearson Similarity on Scaled Purchase Counts']\n",
      "execution_count": 55,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "eval_counts = tc.recommender.util.compare_models(test_data, models_w_counts, model_names=names_w_counts)\neval_dummy = tc.recommender.util.compare_models(test_data_dummy, models_w_dummy, model_names=names_w_dummy)\neval_norm = tc.recommender.util.compare_models(test_data_norm, models_w_norm, model_names=names_w_norm)",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": "PROGRESS: Evaluate model Popularity Model on Purchase Counts\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'evaluate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-a04d8eaa8c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_w_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames_w_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_dummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_dummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_w_dummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames_w_dummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meval_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_w_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames_w_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/turicreate/toolkits/recommender/util.py\u001b[0m in \u001b[0;36mcompare_models\u001b[0;34m(dataset, models, model_names, user_sample, metric, target, exclude_known_for_precision_recall, make_plot, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PROGRESS: Evaluate model %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         r = m.evaluate(\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mdataset_subset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'evaluate'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}